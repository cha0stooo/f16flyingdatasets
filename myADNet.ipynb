{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myADNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0VehCPKBeAMwpsWOASkX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cha0stooo/f16flyingdatasets/blob/master/myADNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tZV0HPD1Dj8",
        "outputId": "eb39375d-fa04-4433-a753-bda5645fbd98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!git clone https://github.com/cha0stooo/f16flyingdatasets.git\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'f16flyingdatasets'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 14 (delta 5), reused 0 (delta 0), pack-reused 0\n",
            "Unpacking objects: 100% (14/14), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUFOTEDF1cIi",
        "outputId": "a790264f-3e0a-4469-f234-4a829f086b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import scipy.io\n",
        "import time\n",
        "tf.random.set_seed(1)\n",
        "np.random.seed(1)\n",
        "LAM = 0.01\n",
        "def getReLUDx(X):\n",
        "    one = tf.ones_like(X)\n",
        "    zero = tf.zeros_like(X)\n",
        "    dX = tf.where(X>=0,x=one,y=zero)\n",
        "    return dX\n",
        "\n",
        "class LANet(object):\n",
        "    #def __init__(self, *inputs, layers, input_dim):   \n",
        "    def __init__(self, inputs, layers, ld_dim,lo_dim):       \n",
        "        self.layers = layers       \n",
        "        self.num_layers = len(self.layers)\n",
        "        self.ld_dim = ld_dim\n",
        "        self.lo_dim = lo_dim\n",
        "\n",
        "        #sample_num = inputs.shape[0]\n",
        "\n",
        "        X = inputs #np.concatenate(inputs, 1)\n",
        "        self.X_mean = X.mean(0, keepdims=True)\n",
        "        self.X_std = X.std(0, keepdims=True)\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.bias_d = []\n",
        "\n",
        "        for l in range(0,self.num_layers-1):\n",
        "            in_dim = self.layers[l]\n",
        "            out_dim = self.layers[l+1]\n",
        "            #W = np.random.normal(size=[out_dim,in_dim])\n",
        "            #He 初始化\n",
        "            W =  np.random.randn(out_dim, in_dim) * np.sqrt(2 / in_dim)\n",
        "            b = np.zeros([out_dim,1])\n",
        "            #g = np.ones([1, out_dim])\n",
        "            # tensorflow variables\n",
        "            self.weights.append(tf.Variable(W, dtype=tf.float64, trainable=True))\n",
        "            self.biases.append(tf.Variable(b, dtype=tf.float64, trainable=True))\n",
        "\n",
        "        #定义一个ld的偏置\n",
        "        bias_d = np.random.rand(1,3)\n",
        "        self.bias_d.append(tf.Variable(bias_d, dtype=tf.float64, trainable=True))\n",
        "\n",
        "    @tf.function     \n",
        "    def __call__(self, *inputs):\n",
        "        \n",
        "        inputs = tf.concat(inputs, 1)\n",
        "        H = inputs\n",
        "        old_dhd0 = tf.eye(self.weights[0].shape[1], dtype=tf.float64)\n",
        "        old_dhd0 = tf.expand_dims(old_dhd0,axis=0)\n",
        "        old_dhd0 = tf.tile(old_dhd0,(H.shape[0],1,1))\n",
        "\n",
        "        for l in range(0, self.num_layers-1):\n",
        "            W = self.weights[l]\n",
        "            b = self.biases[l]\n",
        "\n",
        "            #1.将输入扩展为 样本数X维度X1\n",
        "            H = tf.reshape(H,[H.shape[0],H.shape[1],1])\n",
        "            #2.扩展权重矩阵为 样本数X输出维度X输入维度\n",
        "            W = tf.expand_dims(W,axis=0)\n",
        "            W = tf.tile(W,(H.shape[0],1,1))\n",
        "\n",
        "            #3.扩展偏置向量 样本数X输出维度X1\n",
        "            b = tf.expand_dims(b,axis=0)\n",
        "            b = tf.tile(b,(H.shape[0],1,1))\n",
        "\n",
        "            X = W@H+b       #f(X)  X=wh+b\n",
        "            if l < self.num_layers-2: #若不是最后一层，则用relu激活函数\n",
        "                H = tf.keras.activations.relu(X)   #NXdimX1\n",
        "                dH = getReLUDx(X)                  #NXdimX1\n",
        "                dH = tf.squeeze(dH,axis=2)     #NXdim     \n",
        "                dH_diag = tf.linalg.diag(dH)\n",
        "                dHdH0 = dH_diag@W   #tf.linalg.diag(dH)    NXdimXdim\n",
        "                old_dhd0 = dHdH0@old_dhd0\n",
        "                \n",
        "            else:\n",
        "                #lo层，线性激活函数\n",
        "                X_lo = X[:,0:6,:]   #X_lo = X[:,0:self.lo_dim,:]\n",
        "                H_lo = X_lo\n",
        "                #dHdH0 = tf.eye(len(W))@W\n",
        "                dHdH0_lo = W[:,0:6,:] #dHdH0_lo = W[:,0:self.lo_dim,:]\n",
        "                old_dhd0_lo = dHdH0_lo@old_dhd0\n",
        "\n",
        "                #Ld层，Relu激活函数\n",
        "                # X_ld = X[:,self.lo_dim:self.lo_dim+3,:]\n",
        "                # H_ld = tf.keras.activations.relu(X_ld)\n",
        "                # dH_ld = getReLUDx(X_ld)                  #NXdimX1\n",
        "                # dH_ld = tf.squeeze(dH_ld,axis=2)     #NXdim     \n",
        "                # dH_diag_ld = tf.linalg.diag(dH_ld)\n",
        "                # dHdH0_ld = dH_diag_ld@W[:,self.lo_dim:self.lo_dim+3,:]   #tf.linalg.diag(dH)    NXdimXdim\n",
        "                # old_dhd0_ld = dHdH0_ld@old_dhd0\n",
        "                \n",
        "        Y_lo = tf.squeeze(H_lo,axis=2)   #NXdim\n",
        "        dY_lo = old_dhd0_lo\n",
        "\n",
        "        # Y_ld = tf.squeeze(H_ld,axis=2)+self.bias_d[0]   #NXdim\n",
        "        # dY_ld = old_dhd0_ld\n",
        "        lo = Y_lo[:,0:self.lo_dim]\n",
        "        ld = Y_lo[:,self.lo_dim:self.lo_dim+3]+self.bias_d[0]\n",
        "\n",
        "        d_lo = dY_lo[:,0:self.lo_dim,:]\n",
        "        d_ld = dY_lo[:,self.lo_dim:self.lo_dim+3,:]\n",
        "        return lo,d_lo,ld,d_ld\n",
        "\n",
        "    def save_model(self,path=''):\n",
        "        #保存模型参数\n",
        "        W = self.weights\n",
        "        b = self.biases\n",
        "        bd = self.bias_d\n",
        "   \n",
        "        W_dict = {}\n",
        "        for i in range(len(W)):\n",
        "            W_dict['w'+str(i)] = W[i].numpy()\n",
        "\n",
        "        b_dict = {}\n",
        "        for i in range(len(W)):\n",
        "            b_dict['b'+str(i)] = b[i].numpy()\n",
        "        \n",
        "        bd_dict = {}\n",
        "        bd_dict['bd'] = bd[0].numpy()\n",
        "\n",
        "        total_dict = {}\n",
        "        total_dict.update(W_dict)\n",
        "        total_dict.update(b_dict)\n",
        "        total_dict.update(bd_dict)\n",
        "\n",
        "        scipy.io.savemat('model_paras.mat' , total_dict)\n",
        "\n",
        "        from google.colab import files\n",
        "        files.download('model_paras.mat')\n",
        "\n",
        "    @tf.function\n",
        "    def get_l2_loss(self):\n",
        "        W = self.weights\n",
        "        l2_loss = 0\n",
        "        for _,wi in enumerate(W):\n",
        "            l2_loss = l2_loss+tf.norm(wi)**2\n",
        "        return l2_loss\n",
        "\n",
        "class vehicle_net():\n",
        "    def __init__(self,inputs,layers,ld_dim,lo_dim,hp):\n",
        "        \n",
        "        self.epochs = hp[\"epochs\"]\n",
        "        self.tf_optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=hp[\"tf_lr\"],\n",
        "            beta_1=hp[\"tf_b1\"],\n",
        "            epsilon=hp[\"tf_eps\"])\n",
        "        self.LAM = hp[\"lamda\"]\n",
        "        #self.batchsize = hp[\"batchsize\"]\n",
        "\n",
        "        self.model = LANet(inputs,layers,ld_dim,lo_dim)\n",
        "    @tf.function\n",
        "    def cal_inverse_dynamic(self,q,dq,ddq):\n",
        "        q = tf.convert_to_tensor(q)\n",
        "        dq = tf.convert_to_tensor(dq)   \n",
        "        dq = tf.reshape(dq,[dq.shape[0],dq.shape[1],1])  #NX3 -> NX3X1\n",
        "        ddq = tf.convert_to_tensor(ddq)\n",
        "        ddq = tf.reshape(ddq,[ddq.shape[0],ddq.shape[1],1])  #NX3 -> NX3X1\n",
        "\n",
        "        Y_lo,dY_lo,Y_ld,dY_ld = self.model(q) \n",
        "        #计算逆动力学方程中的各项：M = J*ddq-0.5*[dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq ...]+dJ*dq\n",
        "        #1.J = L*L'\n",
        "        L_mat,L_mat_T,J = self.cal_J(Y_lo,Y_ld)\n",
        "\n",
        "        #2.dJ/dt = dLdt@L'+L@dL'dt = dLdt@L'+L@dLdt'\n",
        "        dl11 = tf.squeeze(dY_ld[:,0:1,:]@dq,axis=2)   #NX1X1 -> NX1\n",
        "        dl22 = tf.squeeze(dY_ld[:,1:2,:]@dq,axis=2)\n",
        "        dl33 = tf.squeeze(dY_ld[:,2:3,:]@dq,axis=2)\n",
        "\n",
        "        dl21 = tf.squeeze(dY_lo[:,0:1,:]@dq,axis=2)\n",
        "        dl31 = tf.squeeze(dY_lo[:,1:2,:]@dq,axis=2)\n",
        "        dl32 = tf.squeeze(dY_lo[:,2:3,:]@dq,axis=2)\n",
        "\n",
        "        dl12 = tf.zeros((dl11.shape[0],1),dtype = tf.float64)\n",
        "        dl13 = tf.zeros((dl11.shape[0],1),dtype = tf.float64)\n",
        "        dl23 = tf.zeros((dl11.shape[0],1),dtype = tf.float64)\n",
        "\n",
        "        dl = tf.stack([dl11,dl12,dl13,dl21,dl22,dl23,dl31,dl32,dl33],axis=1)\n",
        "        dl = tf.squeeze(dl,axis=2)\n",
        "        dl_mat = tf.reshape(dl,[dl.shape[0],3,3])\n",
        "\n",
        "        dl_mat_T = tf.transpose(dl_mat,perm=[0,2,1])  #将NX3X3 中第二维和第三维进行转置 \n",
        "\n",
        "        dJ = dl_mat@L_mat_T+L_mat@dl_mat_T\n",
        "\n",
        "        #3.(dq'@J@dq)/dq\n",
        "        dLo_dq1 = dY_lo[:,:,0]     #NX3\n",
        "        dLd_dq1 = dY_ld[:,:,0]\n",
        "        dl_dq1_mat = self.vec2mat(dLd_dq1,dLo_dq1)   #dl_dq1\n",
        "\n",
        "        dLo_dq2 = dY_lo[:,:,1]\n",
        "        dLd_dq2 = dY_ld[:,:,1]\n",
        "        dl_dq2_mat = self.vec2mat(dLd_dq2,dLo_dq2)\n",
        "\n",
        "        dLo_dq3 = dY_lo[:,:,2]\n",
        "        dLd_dq3 = dY_ld[:,:,2]\n",
        "        dl_dq3_mat = self.vec2mat(dLd_dq3,dLo_dq3)\n",
        "\n",
        "        #计算dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq\n",
        "        dq_T = tf.transpose(dq,perm=[0,2,1])    #NX3X1 ->NX1X3\n",
        "        tmp1 = dq_T@(dl_dq1_mat@L_mat_T+L_mat@tf.transpose(dl_dq1_mat,perm=[0,2,1]))@dq   #NX1X1\n",
        "        tmp2 = dq_T@(dl_dq2_mat@L_mat_T+L_mat@tf.transpose(dl_dq2_mat,perm=[0,2,1]))@dq   #NX1X1\n",
        "        tmp3 = dq_T@(dl_dq3_mat@L_mat_T+L_mat@tf.transpose(dl_dq3_mat,perm=[0,2,1]))@dq   #NX1X1\n",
        "        tmp = tf.stack([tmp1,tmp2,tmp3],axis=1)\n",
        "        tmp = tf.squeeze(tmp,axis=3)   #NX3X1X1  -> NX3X1\n",
        "        #-0.5*[dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq ...]   #NX3X1\n",
        "        f2 = -0.5*tmp\n",
        "\n",
        "        #4.计算M = J*ddq-0.5*[dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq ...]+dJ*dq\n",
        "        M_pre = J@ddq+f2+dJ@dq\n",
        "\n",
        "        gama,phi = q[:,0:1],q[:,1:2]\n",
        "        r11 = tf.ones_like(gama)\n",
        "        r12 = tf.zeros_like(gama)\n",
        "        r13 = -tf.sin(phi)\n",
        "        r21 = tf.zeros_like(gama)\n",
        "        r22 = tf.cos(gama)\n",
        "        r23 = tf.sin(gama)*tf.cos(phi)\n",
        "        r31 = tf.zeros_like(gama)\n",
        "        r32 = -tf.sin(gama)\n",
        "        r33 = tf.cos(gama)*tf.cos(phi)\n",
        "\n",
        "        R = tf.stack([r11,r12,r13,r21,r22,r23,r31,r32,r33],axis=1)\n",
        "        R = tf.squeeze(R,axis=2)\n",
        "        R = tf.reshape(R,[R.shape[0],3,3])\n",
        "        R_T = tf.transpose(R,perm=[0,2,1])  #将NX3X3 中第二维和第三维进行转置\n",
        "\n",
        "        M_pre = tf.linalg.inv(R_T)@M_pre\n",
        "\n",
        "        return M_pre    #NX3X1\n",
        "\n",
        "    @tf.function\n",
        "    def vec2mat(self,dLd_dq1,dLo_dq1):\n",
        "        #将N个3X1的dld_dqi 以及dlo_dqi 组合成 N个 3X3的矩阵\n",
        "        dl_dq11 = dLd_dq1[:,0:1]\n",
        "        dl_dq12 = tf.zeros((dl_dq11.shape[0],1),dtype = tf.float64)\n",
        "        dl_dq13 = tf.zeros((dl_dq11.shape[0],1),dtype = tf.float64)\n",
        "\n",
        "        dl_dq21 = dLo_dq1[:,0:1]\n",
        "        dl_dq22 = dLd_dq1[:,1:2]\n",
        "        dl_dq23 = tf.zeros((dl_dq11.shape[0],1),dtype = tf.float64)\n",
        "\n",
        "        dl_dq31 = dLo_dq1[:,1:2]\n",
        "        dl_dq32 = dLo_dq1[:,2:3]\n",
        "        dl_dq33 = dLd_dq1[:,2:3]\n",
        "\n",
        "        dl = tf.stack([dl_dq11,dl_dq12,dl_dq13,dl_dq21,dl_dq22,dl_dq23,dl_dq31,dl_dq32,dl_dq33],axis=1)\n",
        "        dl = tf.squeeze(dl,axis=2)\n",
        "        dl_dq_mat = tf.reshape(dl,[dl.shape[0],3,3])\n",
        "\n",
        "        return dl_dq_mat\n",
        "\n",
        "    @tf.function  \n",
        "    def cal_J(self,Lo,Ld):\n",
        "       \n",
        "        l1 = Ld[:,0:1]\n",
        "        l2 = tf.zeros((l1.shape[0],1),dtype = tf.float64)\n",
        "        l3 = tf.zeros((l1.shape[0],1),dtype = tf.float64)\n",
        "        l4 = Lo[:,0:1]\n",
        "        l5 = Ld[:,1:2]\n",
        "        l6 = tf.zeros((l1.shape[0],1),dtype = tf.float64)\n",
        "        l7 = Lo[:,1:2]\n",
        "        l8 = Lo[:,2:3]\n",
        "        l9 = Ld[:,2:3]\n",
        "\n",
        "        L_vec = tf.stack([l1,l2,l3,l4,l5,l6,l7,l8,l9],axis=1)\n",
        "        L_vec = tf.squeeze(L_vec,axis=2)\n",
        "        L_mat = tf.reshape(L_vec,[L_vec.shape[0],3,3])\n",
        "\n",
        "        L_vec_T = tf.stack([l1,l4,l7,l2,l5,l8,l3,l6,l9],axis=1)\n",
        "        L_vec_T = tf.squeeze(L_vec_T,axis=2)\n",
        "        L_mat_T = tf.reshape(L_vec_T,[L_vec_T.shape[0],3,3])\n",
        "\n",
        "        J_mat = L_mat@L_mat_T\n",
        "\n",
        "        #J_vec_total = tf.reshape(J_mat,[J_mat.shape[0],9])\n",
        "        return L_mat,L_mat_T,J_mat  \n",
        "\n",
        "    @tf.function\n",
        "    def train(self,q,dq,ddq,mom0):\n",
        "\n",
        "        with tf.GradientTape() as tp:\n",
        "            M_pre0 = self.cal_inverse_dynamic(q,dq,ddq)\n",
        "            #M_pre = tf.squeeze(M_pre0,axis=2)\n",
        "\n",
        "            #四种计loss方式等价\n",
        "            #loss = (tf.keras.losses.MSE(M_pre[:,0],mom[:,0])\\\n",
        "                # +tf.keras.losses.MSE(M_pre[:,1],mom[:,1])\\\n",
        "                # +tf.keras.losses.MSE(M_pre[:,2],mom[:,2]))/3\n",
        "\n",
        "            #loss = tf.reduce_mean(tf.keras.losses.MSE(mom,M_pre))\n",
        "            loss = tf.reduce_mean(tf.keras.losses.MSE(mom0,M_pre0))+LAM*self.model.get_l2_loss()\n",
        "            # loss0 = tf.norm(M_pre[:,0]-mom[:,0])**2/samp_num + \\\n",
        "            #     tf.norm(M_pre[:,1]-mom[:,1])**2/samp_num+\\\n",
        "            #         tf.norm(M_pre[:,2]-mom[:,2])**2/samp_num\n",
        "\n",
        "        variables = self.model.weights+self.model.biases+self.model.bias_d\n",
        "        grads = tp.gradient(loss,variables)\n",
        "        self.tf_optimizer.apply_gradients(grads_and_vars=zip(grads,variables))\n",
        "        del tp\n",
        "        #print(epoch,loss.numpy())\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self,q,dq,ddq,Mom):     \n",
        "        loss_save = []\n",
        "        N_data = q.shape[0]\n",
        "        for ep in range(self.epochs):\n",
        "            #idx_data = np.random.choice(N_data, min(mini_batch_size, N_data))\n",
        "            qi   = q#[idx_data,:]\n",
        "            dqi  = dq#[idx_data,:]\n",
        "            ddqi = ddq#[idx_data,:]\n",
        "            Momi = Mom#[idx_data,:]\n",
        "            Momi = tf.reshape(Momi,[Momi.shape[0],Momi.shape[1],1])\n",
        "            time1 = time.time()                           \n",
        "            loss = self.train(qi,dqi,ddqi,Momi)                \n",
        "            loss_numpy = loss.numpy()\n",
        "            loss_save.append(loss_numpy) \n",
        "            time2 = time.time()\n",
        "            tt1 = time2-time1\n",
        "            if ep%100 == 0:\n",
        "                tf.print(tt1,ep,loss_numpy)\n",
        "        return loss_save\n",
        "\n",
        "def main():\n",
        "    #1.数据准备\n",
        "    state_roll = np.loadtxt('./f16flyingdatasets/state_roll2020-9-16.dat')\n",
        "    state_pitch = np.loadtxt('./f16flyingdatasets/state_pitch2020-9-16.dat')\n",
        "    state_yaw = np.loadtxt('./f16flyingdatasets/state_yaw2020-9-16.dat')\n",
        "    state = np.vstack((state_roll,state_pitch,state_yaw))\n",
        "\n",
        "    state = state_roll\n",
        "\n",
        "    q = state[:,0:3]\n",
        "    dq = state[:,3:6]\n",
        "    ddq = state[:,6:9]\n",
        "    mom = state[:,9:12]\n",
        "    #mom0 = tf.reshape(mom,[mom.shape[0],mom.shape[1],1])\n",
        "    #2.网络初始化\n",
        "    layers = [3]+9*[9]+[9]\n",
        "    hp = {}\n",
        "    hp[\"epochs\"] = 50000\n",
        "    hp[\"tf_lr\"] = 0.01\n",
        "    hp[\"tf_b1\"] = 0.9\n",
        "    hp[\"tf_eps\"] = 0.000000001\n",
        "    hp[\"lamda\"] = 0.01\n",
        "    #hp[\"batchsize\"] = q.shape[0]\n",
        "    vnet = vehicle_net(q,layers,3,3,hp)\n",
        "    \n",
        "    #3.训练\n",
        "    losses = vnet.fit(q,dq,ddq,mom)\n",
        "\n",
        "    #保存结果\n",
        "    #np.savetxt('loss.dat',losses)\n",
        "    path = ''\n",
        "    vnet.model.save_model(path)\n",
        "\n",
        "\n",
        "main()\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.668675422668457 0 1517414984.693989\n",
            "0.008353948593139648 100 1459860.938846928\n",
            "0.008254051208496094 200 1033323.7602475117\n",
            "0.011938095092773438 300 887754.4109884168\n",
            "0.008496522903442383 400 796918.957335403\n",
            "0.008621454238891602 500 732773.2915850626\n",
            "0.008372783660888672 600 686798.5152995144\n",
            "0.008487224578857422 700 658836.0403809\n",
            "0.008381843566894531 800 629593.5664172362\n",
            "0.00848531723022461 900 604707.6997797583\n",
            "0.008999347686767578 1000 580250.941600225\n",
            "0.008699893951416016 1100 552324.0020076266\n",
            "0.008691549301147461 1200 513259.9914763403\n",
            "0.008586645126342773 1300 422061.47062677477\n",
            "0.008472681045532227 1400 131313.02079494574\n",
            "0.010470151901245117 1500 142499.06554634892\n",
            "0.008379936218261719 1600 42176.77639942323\n",
            "0.008635282516479492 1700 41275.25563878283\n",
            "0.008513689041137695 1800 96849.28749070039\n",
            "0.00843667984008789 1900 225577.07109041113\n",
            "0.008435964584350586 2000 29260.054687026513\n",
            "0.008397817611694336 2100 38794.58495152651\n",
            "0.008414030075073242 2200 76952.90229311003\n",
            "0.008714675903320312 2300 284751.7146886283\n",
            "0.008286237716674805 2400 85232.22589793753\n",
            "0.008611202239990234 2500 37514.32974654046\n",
            "0.010479211807250977 2600 44230.83142461979\n",
            "0.008603334426879883 2700 212501.24005058844\n",
            "0.008473634719848633 2800 415557693.0622943\n",
            "0.008413553237915039 2900 719741.0350702608\n",
            "0.008708953857421875 3000 665211.7755468414\n",
            "0.008868217468261719 3100 645148.0405247419\n",
            "0.008312463760375977 3200 632119.0006348803\n",
            "0.008501052856445312 3300 622592.8949415359\n",
            "0.008686542510986328 3400 617030.8151093447\n",
            "0.008265018463134766 3500 610601.7415141006\n",
            "0.008239507675170898 3600 605346.3531156462\n",
            "0.008908510208129883 3700 600861.226186677\n",
            "0.00864553451538086 3800 610614.0451892714\n",
            "0.008691549301147461 3900 606266.6752580957\n",
            "0.00893545150756836 4000 602532.0706408473\n",
            "0.008494138717651367 4100 602466.3053223807\n",
            "0.008406639099121094 4200 615731.420855445\n",
            "0.008460521697998047 4300 612271.9655120802\n",
            "0.008462667465209961 4400 609497.0497703636\n",
            "0.009520769119262695 4500 621397.6334803116\n",
            "0.010712862014770508 4600 617140.9827791197\n",
            "0.008576393127441406 4700 612566.4340599866\n",
            "0.008437633514404297 4800 610428.6247724263\n",
            "0.008430242538452148 4900 608370.2804102678\n",
            "0.008352994918823242 5000 614346.0339348185\n",
            "0.008581161499023438 5100 611796.1189132484\n",
            "0.008633613586425781 5200 609389.893242217\n",
            "0.008468389511108398 5300 606824.9568448612\n",
            "0.008696556091308594 5400 603775.126665312\n",
            "0.008439302444458008 5500 600739.7318711048\n",
            "0.00853419303894043 5600 616569.0453344602\n",
            "0.008359432220458984 5700 609261.7087508856\n",
            "0.008368730545043945 5800 601478.9283896202\n",
            "0.008362054824829102 5900 594401.8668845932\n",
            "0.008529901504516602 6000 590963.0989072175\n",
            "0.008526802062988281 6100 581150.3968595737\n",
            "0.00846242904663086 6200 552278.8879723521\n",
            "0.008858919143676758 6300 491943.6766522423\n",
            "0.008206605911254883 6400 419995.16488098586\n",
            "0.008443355560302734 6500 659721.5575134103\n",
            "0.008344888687133789 6600 173578.4326446691\n",
            "0.008389472961425781 6700 671916.0650136285\n",
            "0.008310556411743164 6800 47818.286272190446\n",
            "0.008420467376708984 6900 60707.71668492919\n",
            "0.008607625961303711 7000 660415.1173810773\n",
            "0.008887767791748047 7100 38659.72069197479\n",
            "0.008507966995239258 7200 38690.57797710105\n",
            "0.008487462997436523 7300 39659.310959400755\n",
            "0.008868932723999023 7400 37617.91404231823\n",
            "0.00862884521484375 7500 40364.817212478454\n",
            "0.008333444595336914 7600 62798.789456821054\n",
            "0.008314371109008789 7700 37669.2811008893\n",
            "0.008589029312133789 7800 385234.2836726669\n",
            "0.00853419303894043 7900 36212.06280740493\n",
            "0.008337736129760742 8000 38466.046575315064\n",
            "0.008358478546142578 8100 220747.20233507015\n",
            "0.008350133895874023 8200 35416.78344999749\n",
            "0.008516073226928711 8300 37982.120368016665\n",
            "0.008520126342773438 8400 35172.445998821306\n",
            "0.008374929428100586 8500 65591.223404735\n",
            "0.008481740951538086 8600 34650.43102979928\n",
            "0.008267402648925781 8700 40858.34010257106\n",
            "0.008594989776611328 8800 34711.70743819018\n",
            "0.008249521255493164 8900 35109.12088432839\n",
            "0.008767366409301758 9000 43903.162016965514\n",
            "0.008245706558227539 9100 35027.54326929883\n",
            "0.008371353149414062 9200 33933.9880350611\n",
            "0.008386850357055664 9300 34686.67554256185\n",
            "0.009595870971679688 9400 44088.334416706515\n",
            "0.00882720947265625 9500 33941.94604340677\n",
            "0.008405447006225586 9600 32347.936178622804\n",
            "0.012588977813720703 9700 32988.893315081405\n",
            "0.008387088775634766 9800 48437.10174211884\n",
            "0.00847005844116211 9900 32767.561330923705\n",
            "0.008535146713256836 10000 2216948.867247942\n",
            "0.010561704635620117 10100 32512.811757600077\n",
            "0.00872945785522461 10200 692767.0371315959\n",
            "0.008538961410522461 10300 31478.17508209313\n",
            "0.008878231048583984 10400 442126.36009545816\n",
            "0.008402109146118164 10500 31025.2485781237\n",
            "0.008448362350463867 10600 52446.670905919316\n",
            "0.008268594741821289 10700 30649.483579194995\n",
            "0.008420705795288086 10800 32194.83597493252\n",
            "0.008298397064208984 10900 30999.071890163526\n",
            "0.00824117660522461 11000 31168.548970800504\n",
            "0.008367300033569336 11100 1906645.195172869\n",
            "0.0084686279296875 11200 30743.16487407026\n",
            "0.008269548416137695 11300 624255.3928053866\n",
            "0.008270978927612305 11400 36206.12143293356\n",
            "0.008309364318847656 11500 177349.71798757336\n",
            "0.008770942687988281 11600 31545.514911536226\n",
            "0.008472442626953125 11700 30621.358397733824\n",
            "0.010385751724243164 11800 29997.913673063755\n",
            "0.008579730987548828 11900 42955.22640666861\n",
            "0.008360862731933594 12000 28709.35780230503\n",
            "0.008574724197387695 12100 30626.70414353195\n",
            "0.008513927459716797 12200 63743.35038907946\n",
            "0.008367538452148438 12300 29018.568774613002\n",
            "0.008208036422729492 12400 59229.780638305296\n",
            "0.00844883918762207 12500 35275.772698614994\n",
            "0.011998176574707031 12600 28959.17683702912\n",
            "0.008341073989868164 12700 96383.85519672518\n",
            "0.008329153060913086 12800 28493.35373467386\n",
            "0.008361339569091797 12900 29920.787209390088\n",
            "0.008438587188720703 13000 1812839.4727943605\n",
            "0.008332014083862305 13100 31257.296903047198\n",
            "0.008488893508911133 13200 458037.7042160877\n",
            "0.00842738151550293 13300 31725.120209024386\n",
            "0.008596420288085938 13400 127554.46183597234\n",
            "0.0084991455078125 13500 30351.709719579532\n",
            "0.008325338363647461 13600 63970.89992033932\n",
            "0.008536100387573242 13700 29261.936775085076\n",
            "0.00844264030456543 13800 132052.59003025838\n",
            "0.00840902328491211 13900 28818.52465603906\n",
            "0.010250329971313477 14000 29779.106882815173\n",
            "0.008524656295776367 14100 45417.26154683922\n",
            "0.008612632751464844 14200 93577.50496914446\n",
            "0.008646249771118164 14300 39093.77216076217\n",
            "0.008805990219116211 14400 35153.56594757446\n",
            "0.008294343948364258 14500 32587.794084505244\n",
            "0.00981283187866211 14600 34548.001910398736\n",
            "0.008389949798583984 14700 280718.9583465095\n",
            "0.008512020111083984 14800 38177.43802371938\n",
            "0.008548498153686523 14900 41716.59604328693\n",
            "0.008565187454223633 15000 946126.379794421\n",
            "0.008402347564697266 15100 37168.49348436496\n",
            "0.00847935676574707 15200 2234861.924829407\n",
            "0.008607149124145508 15300 35366.82111849693\n",
            "0.008623600006103516 15400 51071.453983823056\n",
            "0.008656024932861328 15500 34474.14658957678\n",
            "0.008961915969848633 15600 112096.63871013583\n",
            "0.011502981185913086 15700 34427.3263445536\n",
            "0.00875091552734375 15800 33107.267533072416\n",
            "0.00888371467590332 15900 55669.92564196527\n",
            "0.00885462760925293 16000 751602.1588472284\n",
            "0.008696317672729492 16100 32142.690016250323\n",
            "0.008568763732910156 16200 33676.70735790749\n",
            "0.008786678314208984 16300 31618.77941109072\n",
            "0.008713722229003906 16400 34303.50515592064\n",
            "0.009113550186157227 16500 32204.365130815586\n",
            "0.00856161117553711 16600 36195.48035165424\n",
            "0.008489608764648438 16700 31771.503116104588\n",
            "0.011711359024047852 16800 41603.52391310168\n",
            "0.008541345596313477 16900 31833.932808168676\n",
            "0.008531332015991211 17000 31919.379666285167\n",
            "0.008558273315429688 17100 30982.67275453158\n",
            "0.008329391479492188 17200 30441.474015632564\n",
            "0.00999760627746582 17300 30331.35378454334\n",
            "0.008669614791870117 17400 29691.604455330762\n",
            "0.008557319641113281 17500 68783.07405243009\n",
            "0.008424043655395508 17600 30289.996717426377\n",
            "0.008470535278320312 17700 29868.183956845096\n",
            "0.008560657501220703 17800 105535.26645226931\n",
            "0.010982990264892578 17900 30010.43540459099\n",
            "0.008534669876098633 18000 29549.940003699157\n",
            "0.008640050888061523 18100 67340.26206158838\n",
            "0.008780479431152344 18200 29320.373727754046\n",
            "0.008484125137329102 18300 30268.04183853458\n",
            "0.008456945419311523 18400 29044.149887023723\n",
            "0.008412837982177734 18500 27962.84021154359\n",
            "0.008607625961303711 18600 207844.86122058233\n",
            "0.008360624313354492 18700 25962.888807226478\n",
            "0.00875544548034668 18800 139526.58594623767\n",
            "0.008413553237915039 18900 17549.142212038416\n",
            "0.009317398071289062 19000 55881.091948609355\n",
            "0.008505582809448242 19100 1670784.7976979748\n",
            "0.008225202560424805 19200 7433.973321877757\n",
            "0.008810758590698242 19300 27483.545934438498\n",
            "0.00827646255493164 19400 6279.567571856394\n",
            "0.008315086364746094 19500 5117.585417713031\n",
            "0.008418083190917969 19600 70560.2737321565\n",
            "0.010112524032592773 19700 4033.2321350843663\n",
            "0.009192705154418945 19800 24168.68985392775\n",
            "0.008475065231323242 19900 3231.929394050586\n",
            "0.008362054824829102 20000 2966.2677184049508\n",
            "0.008501768112182617 20100 3806.02663994382\n",
            "0.008714675903320312 20200 2518.879886873484\n",
            "0.008258581161499023 20300 7407.544867557897\n",
            "0.008476972579956055 20400 2329.4141426629185\n",
            "0.008519887924194336 20500 319144.811027349\n",
            "0.008404254913330078 20600 2362.0092362432306\n",
            "0.008542776107788086 20700 2192.6169614635787\n",
            "0.008575201034545898 20800 4932.815288657874\n",
            "0.008567333221435547 20900 23456.017230364636\n",
            "0.008676290512084961 21000 72331.59488036089\n",
            "0.008445262908935547 21100 21849.636655208957\n",
            "0.008474588394165039 21200 7214.075340634418\n",
            "0.008263826370239258 21300 6943.981310295524\n",
            "0.008359670639038086 21400 478946.22434697335\n",
            "0.008404970169067383 21500 4548.259824605479\n",
            "0.008327960968017578 21600 32734.978789777622\n",
            "0.008382797241210938 21700 3459.29089147681\n",
            "0.008521318435668945 21800 7776.886889358261\n",
            "0.009503364562988281 21900 636029.9188192121\n",
            "0.008696794509887695 22000 3729.806039219284\n",
            "0.015934467315673828 22100 142660.8877107671\n",
            "0.010643720626831055 22200 3184.623285382885\n",
            "0.008435726165771484 22300 962360.8603485158\n",
            "0.008565902709960938 22400 22188.492862717972\n",
            "0.008468151092529297 22500 7377.584521506256\n",
            "0.008302450180053711 22600 14411.627121044818\n",
            "0.008661031723022461 22700 14119.439053081243\n",
            "0.01116037368774414 22800 3817.4100405992926\n",
            "0.008472204208374023 22900 4419.752595176323\n",
            "0.008494377136230469 23000 634727.2376634792\n",
            "0.008590221405029297 23100 3181.9843861418553\n",
            "0.008386611938476562 23200 288778.8165953382\n",
            "0.008473873138427734 23300 3120.8868037612137\n",
            "0.008461952209472656 23400 26577.68721460598\n",
            "0.008533477783203125 23500 9872.927826684572\n",
            "0.008898019790649414 23600 13994.14614036179\n",
            "0.008407831192016602 23700 4613.174848786065\n",
            "0.010094165802001953 23800 4719.222336634522\n",
            "0.008589982986450195 23900 7256.159652178971\n",
            "0.00867152214050293 24000 5219.252848853543\n",
            "0.008484363555908203 24100 14946.376758665037\n",
            "0.008661746978759766 24200 26488.148239797567\n",
            "0.008414983749389648 24300 4113.839031518874\n",
            "0.00894618034362793 24400 3961.8898424516356\n",
            "0.008529424667358398 24500 5016.298280913785\n",
            "0.008594989776611328 24600 24662.745256504335\n",
            "0.011948823928833008 24700 11004.995564633247\n",
            "0.008379459381103516 24800 7011.622859295185\n",
            "0.012542486190795898 24900 9051.46850767249\n",
            "0.008455038070678711 25000 13417.651766072619\n",
            "0.008280515670776367 25100 8322.945145696427\n",
            "0.008728981018066406 25200 4447.445355494235\n",
            "0.009190559387207031 25300 9636.174359421204\n",
            "0.008475303649902344 25400 4121.89413335\n",
            "0.008666276931762695 25500 24982.10738452975\n",
            "0.00818634033203125 25600 53784.79639500725\n",
            "0.008519649505615234 25700 15526.424146974203\n",
            "0.00834798812866211 25800 17301.57918978879\n",
            "0.008313655853271484 25900 7606.526566290647\n",
            "0.012152671813964844 26000 6782.580963507477\n",
            "0.008620023727416992 26100 154202.44388498005\n",
            "0.009128808975219727 26200 5530.466874341799\n",
            "0.00850677490234375 26300 6428.9654379130225\n",
            "0.008397579193115234 26400 4546.376066739328\n",
            "0.008749723434448242 26500 6851.962487184182\n",
            "0.008381843566894531 26600 3977.518490060202\n",
            "0.008711814880371094 26700 11800.630640861495\n",
            "0.008578300476074219 26800 3660.8366620853017\n",
            "0.008630037307739258 26900 3415.883341849502\n",
            "0.009468555450439453 27000 355912.8142728885\n",
            "0.008466720581054688 27100 3960.566544269226\n",
            "0.008337736129760742 27200 3028.159292836267\n",
            "0.008413314819335938 27300 4385.972698898684\n",
            "0.00836038589477539 27400 2790.6424679051447\n",
            "0.009189844131469727 27500 13275.333103504805\n",
            "0.008166790008544922 27600 2721.890999885056\n",
            "0.009359359741210938 27700 5885.173609082592\n",
            "0.008833169937133789 27800 7837.503564857478\n",
            "0.009482145309448242 27900 15089.111589945007\n",
            "0.008556604385375977 28000 85587.03696267016\n",
            "0.008472442626953125 28100 16897.7782890997\n",
            "0.008353948593139648 28200 162626.46935859756\n",
            "0.008460760116577148 28300 138672.48979093652\n",
            "0.008634805679321289 28400 3212.587990699596\n",
            "0.012397527694702148 28500 12859.245670454064\n",
            "0.008399486541748047 28600 8768.03720204112\n",
            "0.00876164436340332 28700 3406.819974782589\n",
            "0.008702993392944336 28800 19385.265187687746\n",
            "0.00855255126953125 28900 30227.14530045375\n",
            "0.011542558670043945 29000 234601.17728762125\n",
            "0.008867263793945312 29100 2790.7054383055633\n",
            "0.008438587188720703 29200 3109.4994422290315\n",
            "0.008285999298095703 29300 11026.273333421228\n",
            "0.008314847946166992 29400 10949.466411983685\n",
            "0.008759260177612305 29500 12683.120162408091\n",
            "0.008469820022583008 29600 2672.3405178389303\n",
            "0.00863194465637207 29700 6278.865016351949\n",
            "0.008848905563354492 29800 2966.3856316714723\n",
            "0.008475542068481445 29900 9551.78384306304\n",
            "0.008769512176513672 30000 4192.3346555011685\n",
            "0.008611679077148438 30100 63149.11809892998\n",
            "0.00854802131652832 30200 29042.096349937576\n",
            "0.008406877517700195 30300 5589.230116737648\n",
            "0.008491754531860352 30400 4282.998086434158\n",
            "0.008459806442260742 30500 4320.986842288012\n",
            "0.00846719741821289 30600 4006.9138651853423\n",
            "0.008493185043334961 30700 19078.55904219005\n",
            "0.008308649063110352 30800 4449.798005598941\n",
            "0.00834345817565918 30900 24161.00367656699\n",
            "0.012337923049926758 31000 4415.175492429171\n",
            "0.008447647094726562 32100 426991.64601024333\n",
            "0.009656190872192383 32200 14059.222016492675\n",
            "0.008610248565673828 32300 354910.5216451711\n",
            "0.008485794067382812 32400 126576.9216472904\n",
            "0.008901119232177734 32500 2788.798550415187\n",
            "0.008536815643310547 32600 25313.14419039547\n",
            "0.008288860321044922 32700 282219.5032384713\n",
            "0.008550882339477539 32800 2951.225665783024\n",
            "0.008649349212646484 32900 65519.17333175676\n",
            "0.008369922637939453 33000 3848.2157808772577\n",
            "0.008638381958007812 33100 3587.681038437398\n",
            "0.008676528930664062 33200 263933.01713619626\n",
            "0.008545637130737305 33300 2898.43346704743\n",
            "0.008563518524169922 33400 8531.465139255477\n",
            "0.00867915153503418 33500 14752.340745778707\n",
            "0.008532285690307617 33600 9394.415333866405\n",
            "0.008600711822509766 33700 7496.386082935887\n",
            "0.008462667465209961 33800 12621.004128984501\n",
            "0.008526325225830078 33900 7023.427303777706\n",
            "0.008417367935180664 34000 6744.337136677606\n",
            "0.008396625518798828 34100 13183.203947310109\n",
            "0.00858449935913086 34200 32349.638448756956\n",
            "0.00839090347290039 34300 14611.149441820338\n",
            "0.008419036865234375 34400 11934.108774287844\n",
            "0.010715007781982422 34500 13887.810292972925\n",
            "0.008594989776611328 34600 32546.23273865652\n",
            "0.008624076843261719 34700 179308.1928876467\n",
            "0.008541345596313477 34800 14554.357278755197\n",
            "0.00873255729675293 34900 9382.263780334408\n",
            "0.008501052856445312 35000 5009.382705376166\n",
            "0.00836634635925293 35100 4438.299711075415\n",
            "0.008664608001708984 35200 4205.199878214019\n",
            "0.008496761322021484 35300 28135.94888328086\n",
            "0.008253812789916992 35400 4572.688337400976\n",
            "0.008562326431274414 35500 5959.407875136251\n",
            "0.00843191146850586 35600 5082.370440833722\n",
            "0.0083770751953125 35700 14271.773050821077\n",
            "0.008347272872924805 35800 3608.0614739373814\n",
            "0.008356094360351562 35900 7840.231468259348\n",
            "0.00851297378540039 36000 31266.635240874177\n",
            "0.008480310440063477 36100 4865.244905843754\n",
            "0.008602142333984375 36200 3322.31795666867\n",
            "0.008452892303466797 36300 49721.2016308286\n",
            "0.008540630340576172 36400 21529.670029266246\n",
            "0.008419275283813477 36500 3890.3692350881092\n",
            "0.008568048477172852 36600 44129.38536832302\n",
            "0.008473634719848633 36700 261530.49681973923\n",
            "0.008700370788574219 36800 2889.39726117183\n",
            "0.008444786071777344 36900 2708.978854599687\n",
            "0.00849294662475586 37000 9366.683339429988\n",
            "0.008479595184326172 37100 137328.21941256535\n",
            "0.008395671844482422 37200 7322.221923628067\n",
            "0.00845193862915039 37300 5472.261092549972\n",
            "0.008957147598266602 37400 42333.61335468727\n",
            "0.008348464965820312 37500 3158.704053312581\n",
            "0.008356094360351562 37700 3051.6815204157733\n",
            "0.008480548858642578 37800 15549.609429944012\n",
            "0.008249044418334961 37900 4354.038048325859\n",
            "0.008737564086914062 38000 30830.709182902898\n",
            "0.008270502090454102 38100 125822.08470735494\n",
            "0.008147954940795898 38200 2970.9554166491703\n",
            "0.008405923843383789 38300 11184.376155035412\n",
            "0.008839130401611328 38400 5851.325099169864\n",
            "0.008649826049804688 38500 312723.21350021503\n",
            "0.008566617965698242 38600 3018.8659563683073\n",
            "0.008370637893676758 38700 36045.53366401616\n",
            "0.008686304092407227 38800 2766.4359291401297\n",
            "0.008309364318847656 38900 15862.079644022206\n",
            "0.008385896682739258 39000 2729.6051022992315\n",
            "0.00840306282043457 39100 56922.06187477345\n",
            "0.009099483489990234 39200 27368.62898427255\n",
            "0.008579492568969727 39300 32229.244335287276\n",
            "0.008395195007324219 39400 20805.83694440224\n",
            "0.008364200592041016 39500 6245.759511369526\n",
            "0.008582115173339844 39600 20278.070587684157\n",
            "0.010302543640136719 39700 8376.802797405368\n",
            "0.010090351104736328 39800 6259.392947958741\n",
            "0.008658647537231445 39900 4518.1503353628905\n",
            "0.008320331573486328 40000 6679.386764579603\n",
            "0.008497476577758789 40100 9391.337684685506\n",
            "0.00861358642578125 40200 11680.509453285862\n",
            "0.008472442626953125 40300 15185.555698288254\n",
            "0.008274316787719727 40400 4346.794208789987\n",
            "0.008305788040161133 40500 3223.0117921995625\n",
            "0.008464574813842773 40600 3748.2869375499367\n",
            "0.012333154678344727 40700 3169.4051167542393\n",
            "0.00840616226196289 40800 8379.81667456134\n",
            "0.00844430923461914 40900 3946.1932014799168\n",
            "0.008599281311035156 41000 2764.74014908719\n",
            "0.008360147476196289 41100 9828.403874801219\n",
            "0.008268356323242188 41200 3416.127659017255\n",
            "0.008509159088134766 41300 5873.705714864085\n",
            "0.008515596389770508 41400 38038.21443519915\n",
            "0.008597612380981445 41500 13406.484982444414\n",
            "0.008422374725341797 41600 72628.19252332384\n",
            "0.008274555206298828 41700 11060.59134871722\n",
            "0.008666276931762695 41800 47215.66097501694\n",
            "0.008546113967895508 41900 3229.7023014317188\n",
            "0.008522987365722656 42000 3253.380024195906\n",
            "0.008542299270629883 42100 17074.753310640997\n",
            "0.008662939071655273 42200 23423.6787838575\n",
            "0.00848531723022461 42300 3619.7565725171066\n",
            "0.010310173034667969 42400 3296.598017170287\n",
            "0.008407354354858398 42500 2429.5447324051547\n",
            "0.008565902709960938 42600 17904.851040392998\n",
            "0.008281707763671875 42700 40042.97216493589\n",
            "0.008598566055297852 42800 23837.00373424458\n",
            "0.008542776107788086 42900 6302.278873911048\n",
            "0.008247613906860352 43000 2668.073725760664\n",
            "0.008346319198608398 43100 2629.015366833858\n",
            "0.008202314376831055 43200 9111.805630536533\n",
            "0.008682012557983398 43300 93525.17664419922\n",
            "0.008543968200683594 43400 2710.189574305531\n",
            "0.008584260940551758 43500 4724.785679616064\n",
            "0.008665323257446289 43600 3948.4774300819395\n",
            "0.008242130279541016 43700 3037.8711856942177\n",
            "0.00872039794921875 43800 4794.2055347830365\n",
            "0.008464336395263672 43900 13307.992971136977\n",
            "0.008361339569091797 44000 3221.6207660254354\n",
            "0.008302927017211914 44100 2749.54664635324\n",
            "0.008769989013671875 44200 18696.520831883165\n",
            "0.00850534439086914 44300 308887.9313171598\n",
            "0.008270025253295898 44400 4660.817399608127\n",
            "0.008272886276245117 44500 4626.54650002087\n",
            "0.008507251739501953 44600 2766.474923703636\n",
            "0.008368968963623047 44700 3392.6439271329173\n",
            "0.01089024543762207 44800 207870.38403672786\n",
            "0.008490800857543945 44900 2571.9775809953194\n",
            "0.008315801620483398 45000 6506.152099168928\n",
            "0.008692741394042969 45100 2980.29526575222\n",
            "0.008330106735229492 45200 5483.9424805167455\n",
            "0.008565187454223633 45300 672437.6626336273\n",
            "0.008469581604003906 45400 13156.341428078093\n",
            "0.008588075637817383 45500 20060.518088730034\n",
            "0.008682966232299805 45600 23357.730834133363\n",
            "0.008495092391967773 45700 3609.433116480857\n",
            "0.008402347564697266 45800 11009.704330661054\n",
            "0.008251190185546875 45900 4047.7157784158426\n",
            "0.00861358642578125 46000 41268.65107114119\n",
            "0.008252620697021484 46100 5351.6362219773255\n",
            "0.008714437484741211 46200 2707.0003893777057\n",
            "0.008594751358032227 46300 103954.7453205969\n",
            "0.00854039192199707 46400 13998.93430302404\n",
            "0.008546590805053711 46500 3534.257765803773\n",
            "0.008247852325439453 46600 25781.985924348144\n",
            "0.008841514587402344 46700 11444.797164053336\n",
            "0.010154485702514648 46800 5428.57505623958\n",
            "0.008603334426879883 46900 4982.597857660875\n",
            "0.008329391479492188 47000 16197.278079194599\n",
            "0.010425090789794922 47100 21918.714529626133\n",
            "0.008388519287109375 47200 2570.971929834671\n",
            "0.008774042129516602 47300 8350.355628777465\n",
            "0.008972883224487305 47400 28798.689663590645\n",
            "0.008205890655517578 47500 46310.269260072724\n",
            "0.008540153503417969 47600 14719.415145133737\n",
            "0.008472681045532227 47700 9892.705177043395\n",
            "0.0087738037109375 47800 4931.50110858352\n",
            "0.008421659469604492 47900 38623.76339806926\n",
            "0.009577512741088867 48000 2161.471178320001\n",
            "0.008624792098999023 48100 3143.391519900053\n",
            "0.00845646858215332 48200 3434.523512356541\n",
            "0.00823211669921875 48300 2655.388628016616\n",
            "0.008586883544921875 48400 9429.307799477736\n",
            "0.008486747741699219 48500 261507.30636560387\n",
            "0.00828242301940918 48600 506265.96683109587\n",
            "0.008521080017089844 48700 1905.0258559408085\n",
            "0.008567571640014648 48800 31814.517389871406\n",
            "0.00873565673828125 48900 139838.72256170635\n",
            "0.008543968200683594 49000 2025.7678379649847\n",
            "0.00836944580078125 49100 3548.5742710885775\n",
            "0.008598089218139648 49200 2885.5702089833726\n",
            "0.01179051399230957 49300 2570.963361748493\n",
            "0.008384466171264648 49400 2028.6555983857488\n",
            "0.008651256561279297 49500 7939.481211530048\n",
            "0.008975505828857422 49600 110218.59387192184\n",
            "0.008934497833251953 49700 57676.47913560912\n",
            "0.008900165557861328 49800 271233.5110952438\n",
            "0.010063648223876953 49900 1963.4015048788747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_64129fad-02f6-435c-99b2-6a66376e8adf\", \"model_paras.mat\", 8096)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}