{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lagrange_net",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cha0stooo/f16flyingdatasets/blob/master/lagrange_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7QW3FJQ7cn",
        "outputId": "6424d98a-07b6-4bbc-9f90-403ac18b6fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!ls -R\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".:\n",
            "sample_data\n",
            "\n",
            "./sample_data:\n",
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X3O5JS3LqpZ",
        "outputId": "e0659b5a-83df-4fb9-851d-2de6652eb8ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "class MyModel(Model):\n",
        "  def __init__(self,layers):  \n",
        "    super(MyModel, self).__init__()\n",
        "    tf.keras.backend.set_floatx(\"float64\")  \n",
        "    self.layer_list = []\n",
        "    self.bn = []\n",
        "    \n",
        "    for width in layers[1:-2]:\n",
        "        self.layer_list.append(Dense(width, activation='relu',kernel_initializer=tf.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "        self.bn.append(BatchNormalization())\n",
        "    \n",
        "    #self.layer_list.append(Dense(layers[-1], activation='linear',kernel_initializer= tf.initializers.normal()))        \n",
        "    self.Lo_layer = Dense(layers[-2],activation='linear',kernel_initializer= 'glorot_normal',kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "    self.Ld_layer = Dense(layers[-1],activation='relu',kernel_initializer=tf.initializers.he_normal(),kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "     #定义一个ld的偏置\n",
        "    self.bias_d = []\n",
        "    bias_d = abs(tf.random.normal([1,3],dtype=tf.float64))\n",
        "    self.bias_d.append(tf.Variable(bias_d, dtype=tf.float64, trainable=True))\n",
        "\n",
        "  @tf.function\n",
        "  def call(self, x):\n",
        "    for index in range(len(self.layer_list)):\n",
        "      x = self.layer_list[index](x)\n",
        "      x = self.bn[index](x)\n",
        "        \n",
        "    lo = self.Lo_layer(x)\n",
        "    ld = self.Ld_layer(x)+self.bias_d[0]\n",
        "    \n",
        "    return lo,ld\n",
        "    \n",
        "def save_model(model):\n",
        "  #保存模型参数\n",
        "  # checkpoint = tf.train.Checkpoint(myModel=model)\n",
        "  # checkpoint.save(path+'/model.ckpt')\n",
        "  model.save_weights('weight_tf_savedmodel_h5', save_format='h5')\n",
        "    \n",
        "def restore_model(model,path):\n",
        "  #恢复模型参数\n",
        "  checkpoint = tf.train.Checkpoint(myModel=model)\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(path))\n",
        "\n",
        "#产生飞行器训练数据\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "\n",
        "class vehicle_net():\n",
        "  def __init__(self,layers,hp):\n",
        "    self.epochs = hp[\"epochs\"]\n",
        "    self.tf_optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=hp[\"tf_lr\"],\n",
        "        beta_1=hp[\"tf_b1\"],\n",
        "        epsilon=hp[\"tf_eps\"])\n",
        "    self.LAM = hp[\"lamda\"]\n",
        "    #batchsize = hp['batchsize']\n",
        "    self.model = MyModel(layers=layers)\n",
        "\n",
        "\n",
        "  @tf.function\n",
        "  def cal_inverse_dynamic(self,q,dq,ddq):\n",
        "    q = tf.convert_to_tensor(q)\n",
        "    dq = tf.convert_to_tensor(dq)   \n",
        "    dq = tf.reshape(dq,[dq.shape[0],dq.shape[1],1])  #NX3 -> NX3X1\n",
        "    ddq = tf.convert_to_tensor(ddq)\n",
        "    ddq = tf.reshape(ddq,[ddq.shape[0],ddq.shape[1],1])  #NX3 -> NX3X1\n",
        "    sample_num = q.shape[0]\n",
        "    with tf.GradientTape(persistent=True) as tp:\n",
        "      tp.watch(q) \n",
        "      Y_lo,Y_ld = self.model(q)   \n",
        "      l1 = Y_ld[:,0:1]\n",
        "      l2 = Y_ld[:,1:2]\n",
        "      l3 = Y_ld[:,2:3]\n",
        "      l4 = Y_lo[:,0:1]\n",
        "      l5 = Y_lo[:,1:2]\n",
        "      l6 = Y_lo[:,2:3]\n",
        "      # #计算dLi_dq\n",
        "      dL1_dq = tp.gradient(l1,q)  #NX3\n",
        "      dL2_dq = tp.gradient(l2,q)  #NX3\n",
        "      dL3_dq = tp.gradient(l3,q)  #NX3\n",
        "      dL4_dq = tp.gradient(l4,q)  #NX3\n",
        "      dL5_dq = tp.gradient(l5,q)  #NX3\n",
        "      dL6_dq = tp.gradient(l6,q)  #NX3\n",
        "      #计算dl_dqi\n",
        "      dLd_dq1 = tf.stack([dL1_dq[:,0:1],dL2_dq[:,0:1],dL3_dq[:,0:1]],axis=1)  #NX3X1\n",
        "      dLd_dq1 = tf.squeeze(dLd_dq1,axis=2)  #NX3\n",
        "      dLd_dq2 = tf.stack([dL1_dq[:,1:2],dL2_dq[:,1:2],dL3_dq[:,1:2]],axis=1)\n",
        "      dLd_dq2 = tf.squeeze(dLd_dq2,axis=2)  #NX3  \n",
        "      dLd_dq3 = tf.stack([dL1_dq[:,2:3],dL2_dq[:,2:3],dL3_dq[:,2:3]],axis=1)\n",
        "      dLd_dq3 = tf.squeeze(dLd_dq3,axis=2)  #NX3  \n",
        "      dLo_dq1 = tf.stack([dL4_dq[:,0:1],dL5_dq[:,0:1],dL6_dq[:,0:1]],axis=1)\n",
        "      dLo_dq1 = tf.squeeze(dLo_dq1,axis=2)  #NX3  \n",
        "      dLo_dq2 = tf.stack([dL4_dq[:,1:2],dL5_dq[:,1:2],dL6_dq[:,1:2]],axis=1)  \n",
        "      dLo_dq2 = tf.squeeze(dLo_dq2,axis=2)  #NX3\n",
        "      dLo_dq3 = tf.stack([dL4_dq[:,2:3],dL5_dq[:,2:3],dL6_dq[:,2:3]],axis=1)  \n",
        "      dLo_dq3 = tf.squeeze(dLo_dq3,axis=2)  #NX3\n",
        "    del tp\n",
        "    #计算逆动力学方程中的各项：M = J*ddq-0.5*[dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq ...]+dJ*dq\n",
        "    #1.J = L*L'\n",
        "    L_mat,L_mat_T,J = self.cal_J(Y_lo,Y_ld)\n",
        "    #2.dJ/dt = dLdt@L'+L@dL'dt = dLdt@L'+L@dLdt'\n",
        "    dL1_dq = tf.reshape(dL1_dq,[dL1_dq.shape[0],1,dL1_dq.shape[1]])\n",
        "    dL2_dq = tf.reshape(dL2_dq,[dL2_dq.shape[0],1,dL2_dq.shape[1]])\n",
        "    dL3_dq = tf.reshape(dL3_dq,[dL3_dq.shape[0],1,dL3_dq.shape[1]])\n",
        "    dL4_dq = tf.reshape(dL4_dq,[dL4_dq.shape[0],1,dL4_dq.shape[1]])\n",
        "    dL5_dq = tf.reshape(dL5_dq,[dL5_dq.shape[0],1,dL5_dq.shape[1]])\n",
        "    dL6_dq = tf.reshape(dL6_dq,[dL6_dq.shape[0],1,dL6_dq.shape[1]])\n",
        "    dl11 = tf.squeeze(dL1_dq@dq,axis=2)   #NX1X1 -> NX1\n",
        "    dl22 = tf.squeeze(dL2_dq@dq,axis=2)\n",
        "    dl33 = tf.squeeze(dL3_dq@dq,axis=2)\n",
        "    dl21 = tf.squeeze(dL4_dq@dq,axis=2)\n",
        "    dl31 = tf.squeeze(dL5_dq@dq,axis=2)\n",
        "    dl32 = tf.squeeze(dL6_dq@dq,axis=2)\n",
        "    dl12 = tf.zeros((dl11.shape[0],1),dtype = tf.float64)\n",
        "    dl13 = tf.zeros((dl11.shape[0],1),dtype = tf.float64)\n",
        "    dl23 = tf.zeros((dl11.shape[0],1),dtype = tf.float64)\n",
        "    dl = tf.stack([dl11,dl12,dl13,dl21,dl22,dl23,dl31,dl32,dl33],axis=1)\n",
        "    dl = tf.squeeze(dl,axis=2)\n",
        "    dl_mat = tf.reshape(dl,[dl.shape[0],3,3])\n",
        "    dl_mat_T = tf.transpose(dl_mat,perm=[0,2,1])  #将NX3X3 中第二维和第三维进行转置 \n",
        "    dJ = dl_mat@L_mat_T+L_mat@dl_mat_T\n",
        "    #3.(dq'@J@dq)/dq\n",
        "    #dLo_dq1 = dY_lo[:,:,0]     #NX3\n",
        "    #dLd_dq1 = dY_ld[:,:,0]\n",
        "    dl_dq1_mat = self.vec2mat(dLd_dq1,dLo_dq1)   #dl_dq1\n",
        "    # dLo_dq2 = dY_lo[:,:,1]\n",
        "    # dLd_dq2 = dY_ld[:,:,1]\n",
        "    dl_dq2_mat = self.vec2mat(dLd_dq2,dLo_dq2)\n",
        "    # dLo_dq3 = dY_lo[:,:,2]\n",
        "    # dLd_dq3 = dY_ld[:,:,2]\n",
        "    dl_dq3_mat = self.vec2mat(dLd_dq3,dLo_dq3)\n",
        "    #计算dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq\n",
        "    dq_T = tf.transpose(dq,perm=[0,2,1])    #NX3X1 ->NX1X3\n",
        "    tmp1 = dq_T@(dl_dq1_mat@L_mat_T+L_mat@tf.transpose(dl_dq1_mat,perm=[0,2,1]))@dq   #NX1X1\n",
        "    tmp2 = dq_T@(dl_dq2_mat@L_mat_T+L_mat@tf.transpose(dl_dq2_mat,perm=[0,2,1]))@dq   #NX1X1\n",
        "    tmp3 = dq_T@(dl_dq3_mat@L_mat_T+L_mat@tf.transpose(dl_dq3_mat,perm=[0,2,1]))@dq   #NX1X1\n",
        "    tmp = tf.stack([tmp1,tmp2,tmp3],axis=1)\n",
        "    tmp = tf.squeeze(tmp,axis=3)   #NX3X1X1  -> NX3X1\n",
        "    #-0.5*[dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq ...]   #NX3X1\n",
        "    f2 = -0.5*tmp\n",
        "    #4.计算M = J*ddq-0.5*[dq_T*(dl_dq1*L_T+L*dL_dq1_T)*dq ...]+dJ*dq\n",
        "    M_pre0 = J@ddq+f2+dJ@dq\n",
        "    gama,phi = q[:,0:1],q[:,1:2]\n",
        "    r11 = tf.ones_like(gama)\n",
        "    r12 = tf.zeros_like(gama)\n",
        "    r13 = -tf.sin(phi)\n",
        "    r21 = tf.zeros_like(gama)\n",
        "    r22 = tf.cos(gama)\n",
        "    r23 = tf.sin(gama)*tf.cos(phi)\n",
        "    r31 = tf.zeros_like(gama)\n",
        "    r32 = -tf.sin(gama)\n",
        "    r33 = tf.cos(gama)*tf.cos(phi)\n",
        "    R = tf.stack([r11,r12,r13,r21,r22,r23,r31,r32,r33],axis=1)\n",
        "    R = tf.squeeze(R,axis=2)\n",
        "    R = tf.reshape(R,[R.shape[0],3,3])\n",
        "    R_T = tf.transpose(R,perm=[0,2,1])  #将NX3X3 中第二维和第三维进行转置\n",
        "    M_pre = tf.linalg.inv(R_T)@M_pre0\n",
        "    return M_pre0    #NX3X1\n",
        "\n",
        "  @tf.function\n",
        "  def vec2mat(self,dLd_dq1,dLo_dq1):\n",
        "    #将N个3X1的dld_dqi 以及dlo_dqi 组合成 N个 3X3的矩阵\n",
        "    dl_dq11 = dLd_dq1[:,0:1]\n",
        "    dl_dq12 = tf.zeros((dl_dq11.shape[0],1),dtype = tf.float64)\n",
        "    dl_dq13 = tf.zeros((dl_dq11.shape[0],1),dtype = tf.float64)\n",
        "    dl_dq21 = dLo_dq1[:,0:1]\n",
        "    dl_dq22 = dLd_dq1[:,1:2]\n",
        "    dl_dq23 = tf.zeros((dl_dq11.shape[0],1),dtype = tf.float64)\n",
        "    dl_dq31 = dLo_dq1[:,1:2]\n",
        "    dl_dq32 = dLo_dq1[:,2:3]\n",
        "    dl_dq33 = dLd_dq1[:,2:3]\n",
        "    dl = tf.stack([dl_dq11,dl_dq12,dl_dq13,dl_dq21,dl_dq22,dl_dq23,dl_dq31,dl_dq32,dl_dq33],axis=1)\n",
        "    dl = tf.squeeze(dl,axis=2)\n",
        "    dl_dq_mat = tf.reshape(dl,[dl.shape[0],3,3])\n",
        "    return dl_dq_mat\n",
        "  @tf.function  \n",
        "  def cal_J(self,Lo,Ld): \n",
        "    l1 = Ld[:,0:1]\n",
        "    l2 = tf.zeros((l1.shape[0],1),dtype = tf.float64)\n",
        "    l3 = tf.zeros((l1.shape[0],1),dtype = tf.float64)\n",
        "    l4 = Lo[:,0:1]\n",
        "    l5 = Ld[:,1:2]\n",
        "    l6 = tf.zeros((l1.shape[0],1),dtype = tf.float64)\n",
        "    l7 = Lo[:,1:2]\n",
        "    l8 = Lo[:,2:3]\n",
        "    l9 = Ld[:,2:3]\n",
        "    L_vec = tf.stack([l1,l2,l3,l4,l5,l6,l7,l8,l9],axis=1)\n",
        "    L_vec = tf.squeeze(L_vec,axis=2)\n",
        "    L_mat = tf.reshape(L_vec,[L_vec.shape[0],3,3])\n",
        "    L_vec_T = tf.stack([l1,l4,l7,l2,l5,l8,l3,l6,l9],axis=1)\n",
        "    L_vec_T = tf.squeeze(L_vec_T,axis=2)\n",
        "    L_mat_T = tf.reshape(L_vec_T,[L_vec_T.shape[0],3,3])\n",
        "    J_mat = L_mat@L_mat_T\n",
        "    #J_vec_total = tf.reshape(J_mat,[J_mat.shape[0],9])\n",
        "    return L_mat,L_mat_T,J_mat  \n",
        "  @tf.function\n",
        "  def train(self,q,dq,ddq,mom0):\n",
        "    with tf.GradientTape() as tp:\n",
        "      M_pre0 = self.cal_inverse_dynamic(q,dq,ddq)\n",
        "      #M_pre = tf.squeeze(M_pre0,axis=2)\n",
        "      #四种计loss方式等价\n",
        "      #loss = (tf.keras.losses.MSE(M_pre[:,0],mom[:,0])\\\n",
        "          # +tf.keras.losses.MSE(M_pre[:,1],mom[:,1])\\\n",
        "          # +tf.keras.losses.MSE(M_pre[:,2],mom[:,2]))/3\n",
        "      #loss = tf.reduce_mean(tf.keras.losses.MSE(mom,M_pre))\n",
        "      res_loss = tf.reduce_mean(tf.keras.losses.MSE(mom0,M_pre0))\n",
        "      loss = res_loss#+LAM*self.model.get_l2_loss()\n",
        "      # loss0 = tf.norm(M_pre[:,0]-mom[:,0])**2/samp_num + \\\n",
        "      #     tf.norm(M_pre[:,1]-mom[:,1])**2/samp_num+\\\n",
        "      #         tf.norm(M_pre[:,2]-mom[:,2])**2/samp_num\n",
        "    variables = self.model.trainable_variables\n",
        "    grads = tp.gradient(loss,variables)\n",
        "    self.tf_optimizer.apply_gradients(grads_and_vars=zip(grads,variables))\n",
        "    del tp\n",
        "    #print(epoch,loss.numpy())\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def fit(self,q,dq,ddq,Mom,mini_batch_size=128):     \n",
        "    loss_save = []\n",
        "    N_data = q.shape[0]\n",
        "    for ep in range(self.epochs): \n",
        "      #idx_data = np.random.choice(N_data, min(mini_batch_size, N_data))\n",
        "      qi   = q#[idx_data,:]\n",
        "      dqi  = dq#[idx_data,:]\n",
        "      ddqi = ddq#[idx_data,:]\n",
        "      Momi = Mom#[idx_data,:]\n",
        "      Momi = tf.reshape(Momi,[Momi.shape[0],Momi.shape[1],1])\n",
        "      time1 = time.time()                           \n",
        "      loss = self.train(qi,dqi,ddqi,Momi)                \n",
        "      loss_numpy = loss.numpy()\n",
        "      loss_save.append(loss_numpy) \n",
        "      if ep%100==0:\n",
        "          time2 = time.time()\n",
        "          tt1 = time2-time1\n",
        "          tf.print(tt1,ep,loss_numpy)\n",
        "    return loss_save\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def main():\n",
        "  #1.数据准备\n",
        "  state_roll = np.loadtxt('./f16flyingdatasets/state_roll2020-9-25.dat')\n",
        "  state_pitch = np.loadtxt('./f16flyingdatasets/state_pitch2020-9-25.dat')\n",
        "  state_yaw = np.loadtxt('./f16flyingdatasets/state_yaw2020-9-25.dat')\n",
        "\n",
        "  state = np.vstack((state_roll,state_pitch,state_yaw)) \n",
        "  state = state \n",
        "  q = state[:,0:3]\n",
        "  dq = state[:,3:6]\n",
        "  ddq = state[:,6:9]\n",
        "  mom = state[:,9:12]\n",
        "  #mom0 = tf.reshape(mom,[mom.shape[0],mom.shape[1],1])\n",
        "  #2.网络初始化\n",
        "  layers = [3]+9*[9]+[3]+[3]\n",
        "  hp = {}\n",
        "  hp[\"epochs\"] = 100000\n",
        "  hp[\"tf_lr\"] = 0.005\n",
        "  hp[\"tf_b1\"] = 0.9\n",
        "  hp[\"tf_eps\"] = 0.000000001\n",
        "  hp[\"lamda\"] = 0.01\n",
        "  hp['batchsize'] = q.shape[0]\n",
        "  vnet = vehicle_net(layers=layers,hp=hp)\n",
        "\n",
        "  #3.训练\n",
        "  losses = vnet.fit(q,dq,ddq,mom,mini_batch_size=q.shape[0])  \n",
        "  #保存结果\n",
        "  np.savetxt('./f16flyingdatasets/loss.dat',losses)\n",
        "  #path = 'save/926'\n",
        "  # vnet.model.save_model(path)\n",
        "  #save_model(vnet.model, \"./f16flyingdatasets/save/\")\n",
        "  save_model(vnet.model) \n",
        "  #restore_model(vnet.model,\"saved/\")\n",
        "  files.download('weight_tf_savedmodel_h5')\n",
        "\n",
        "\n",
        "\n",
        "def validate():\n",
        "  import matplotlib.pyplot as plt\n",
        "  #1.数据准备\n",
        "  state_roll = np.loadtxt('./f16flyingdatasets/state_roll2020-9-16.dat')\n",
        "  state_pitch = np.loadtxt('./f16flyingdatasets/state_pitch2020-9-16.dat')\n",
        "  state_yaw = np.loadtxt('./f16flyingdatasets/state_yaw2020-9-16.dat')\n",
        "  state = np.vstack((state_roll,state_pitch,state_yaw))\n",
        "  state = state_roll\n",
        "  q = state[:,0:3]\n",
        "  dq = state[:,3:6]\n",
        "  ddq = state[:,6:9]\n",
        "  mom = state[:,9:12]\n",
        "  mom = tf.reshape(mom,[mom.shape[0],mom.shape[1],1])\n",
        "  #恢复网络\n",
        "  hp = {}\n",
        "  hp[\"epochs\"] = 2000\n",
        "  hp[\"tf_lr\"] = 0.01\n",
        "  hp[\"tf_b1\"] = 0.9\n",
        "  hp[\"tf_eps\"] = 0.000000001\n",
        "  hp[\"lamda\"] = 0.01\n",
        "  #hp[\"batchsize\"] = 1001\n",
        "  layers = [3]+12*[9]+[3]+[3]\n",
        "  vnet = vehicle_net(layers=layers,hp=hp)\n",
        "\n",
        "  vnet.model.load_weights('weight_tf_savedmodel_h5')\n",
        "  #restore_model(vnet.model,\"./f16flyingdatasets/save/\")\n",
        "\n",
        "  t1 = time.time()\n",
        "  mom_pred = vnet.cal_inverse_dynamic(q,dq,ddq)\n",
        "  t2 = time.time()\n",
        "  print(t2-t1)\n",
        "  plt.figure()\n",
        "  #loss = np.loadtxt('loss.dat')\n",
        "  #plt.plot(loss)  \n",
        "  plt.figure()\n",
        "  for fig_id in range(3):\n",
        "      plt.subplot(3,1,fig_id+1)\n",
        "      plt.plot(mom_pred[:,fig_id,:])\n",
        "      plt.plot(mom[:,fig_id,:],'--')\n",
        "\n",
        "  plt.show()\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "  #validate()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.40610122680664 0 13674443596.645185\n",
            "0.01847052574157715 100 207646958.41944325\n",
            "0.018664121627807617 200 179491664.15446705\n",
            "0.01863265037536621 300 175824510.58289298\n",
            "0.01942920684814453 400 174313082.55906504\n",
            "0.01866936683654785 500 173536675.77072683\n",
            "0.018590450286865234 600 173132271.7160028\n",
            "0.019902706146240234 700 172896929.4878741\n",
            "0.018343687057495117 800 172753477.7789039\n",
            "0.01852130889892578 900 172687839.61427695\n",
            "0.01859140396118164 1000 172590832.27799666\n",
            "0.018666982650756836 1100 172502395.77847406\n",
            "0.0185394287109375 1200 172423275.52381682\n",
            "0.01879096031188965 1300 172346353.71167186\n",
            "0.018476247787475586 1400 172270720.46722436\n",
            "0.01859116554260254 1500 172199582.79743204\n",
            "0.01843428611755371 1600 172127477.48384982\n",
            "0.018297910690307617 1700 172057947.00079447\n",
            "0.018551349639892578 1800 171992024.48133036\n",
            "0.023175477981567383 1900 171923605.0864993\n",
            "0.01864337921142578 2000 171857422.67061606\n",
            "0.018581628799438477 2100 171793687.6756429\n",
            "0.018751144409179688 2200 171728027.5770331\n",
            "0.018455028533935547 2300 171663629.32082286\n",
            "0.0187985897064209 2400 171602515.62901607\n",
            "0.018458843231201172 2500 171552428.28484243\n",
            "0.018558502197265625 2600 171483902.09306976\n",
            "0.018739938735961914 2700 171427859.10105482\n",
            "0.018644094467163086 2800 171372313.13612884\n",
            "0.018574953079223633 2900 171320320.5301399\n",
            "0.018689632415771484 3000 171264208.11209822\n",
            "0.019827604293823242 3100 171226558.708435\n",
            "0.01868152618408203 3200 171165437.40398416\n",
            "0.018680810928344727 3300 173163457.77207625\n",
            "0.018538951873779297 3400 171072638.1018149\n",
            "0.018607616424560547 3500 171027862.77605498\n",
            "0.01865243911743164 3600 171063485.2462064\n",
            "0.018646240234375 3700 170943802.94070476\n",
            "0.018630027770996094 3800 170904961.67171034\n",
            "0.018703222274780273 3900 171014939.6368367\n",
            "0.018558263778686523 4000 170827674.54239443\n",
            "0.018459081649780273 4100 170790025.76206464\n",
            "0.018662452697753906 4200 170756593.24998638\n",
            "0.01864933967590332 4300 170719142.41967568\n",
            "0.018495798110961914 4400 175362900.41642293\n",
            "0.021229267120361328 4500 170644997.82754242\n",
            "0.019074201583862305 4600 170601581.8412678\n",
            "0.018208980560302734 4700 170575624.25133216\n",
            "0.019677162170410156 4800 170517390.72315392\n",
            "0.018593549728393555 4900 175439187.39906725\n",
            "0.018479108810424805 5000 170415010.96943504\n",
            "0.018699169158935547 5100 170349403.29404813\n",
            "0.02088165283203125 5200 170355523.22048774\n",
            "0.02253270149230957 5300 170186782.65061188\n",
            "0.01821589469909668 5400 179155679.79591015\n",
            "0.01866006851196289 5500 169909355.46163023\n",
            "0.018683671951293945 5600 169658609.8833458\n",
            "0.01855635643005371 5700 169218240.5684612\n",
            "0.023464441299438477 5800 167756266.4943362\n",
            "0.01866316795349121 5900 1271721.3334273412\n",
            "0.0186307430267334 6000 291761.0130057267\n",
            "0.01873302459716797 6100 264081.853919881\n",
            "0.01884150505065918 6200 274994.35806309705\n",
            "0.018865346908569336 6300 247158.25384237626\n",
            "0.01865530014038086 6400 810621.2110218103\n",
            "0.018459081649780273 6500 239881.73061161372\n",
            "0.018548011779785156 6600 352479.6858432904\n",
            "0.01857161521911621 6700 235925.25658551182\n",
            "0.01879715919494629 6800 1025172.6574287231\n",
            "0.018657684326171875 6900 233473.31283980943\n",
            "0.018419265747070312 7000 232841.7335847328\n",
            "0.018741369247436523 7100 233639.84000208293\n",
            "0.018764495849609375 7200 230545.92440717426\n",
            "0.018589019775390625 7300 235076.83427122305\n",
            "0.018379688262939453 7400 230355.51991700736\n",
            "0.018592357635498047 7500 228508.74682965598\n",
            "0.018654584884643555 7600 227957.59269882867\n",
            "0.019258499145507812 7700 309244.6308395364\n",
            "0.018619775772094727 7800 226952.09066230626\n",
            "0.018810510635375977 7900 634134.8992165227\n",
            "0.01853775978088379 8000 226186.08966151852\n",
            "0.01857757568359375 8100 225312.27719239052\n",
            "0.018683910369873047 8200 312870.24326171307\n",
            "0.018564224243164062 8300 224753.91731263345\n",
            "0.01868748664855957 8400 282742.3782633676\n",
            "0.01876091957092285 8500 226807.1169068526\n",
            "0.018671035766601562 8600 223591.1370385451\n",
            "0.018556594848632812 8700 766680.6552238003\n",
            "0.018958330154418945 8800 223629.99199653248\n",
            "0.018800973892211914 8900 222813.80381734713\n",
            "0.022655725479125977 9000 1045338.9662863343\n",
            "0.018604040145874023 9100 221839.6936018409\n",
            "0.01868605613708496 9200 221324.3396017809\n",
            "0.01856064796447754 9300 224560.71723657683\n",
            "0.018507957458496094 9400 221057.94706806424\n",
            "0.018802165985107422 9500 262172.25573664135\n",
            "0.01859736442565918 9600 225481.07202672356\n",
            "0.019767045974731445 9800 299018.37021166465\n",
            "0.018679141998291016 9900 223577.20623516885\n",
            "0.018629789352416992 10000 226070.44533919357\n",
            "0.018757104873657227 10100 290839.63926937134\n",
            "0.018401622772216797 10200 222988.41921858396\n",
            "0.01852726936340332 10300 223986.808128387\n",
            "0.01878666877746582 10400 222115.86592871154\n",
            "0.01877117156982422 10500 253190.80529859176\n",
            "0.018583059310913086 10600 221535.69702276145\n",
            "0.018636226654052734 10700 344924.433026079\n",
            "0.018752098083496094 10800 222652.3287632365\n",
            "0.02328324317932129 10900 2342986.4382265876\n",
            "0.02340555191040039 11000 220810.28519135393\n",
            "0.020094871520996094 11100 1850523.4116423377\n",
            "0.01862931251525879 11200 220559.80860201278\n",
            "0.018749237060546875 11300 392502.0988285122\n",
            "0.018579483032226562 11400 219852.9270563531\n",
            "0.018701791763305664 11500 235924.13336472894\n",
            "0.018787622451782227 11600 219745.8160930819\n",
            "0.01871204376220703 11700 12117420.640060991\n",
            "0.01854729652404785 11800 219734.22411920124\n",
            "0.01880192756652832 11900 829116.7009423563\n",
            "0.01869368553161621 12000 219635.58160259653\n",
            "0.018597841262817383 12100 253159.0758606341\n",
            "0.018713951110839844 12200 219255.77549109384\n",
            "0.01891779899597168 12300 283511.98465576145\n",
            "0.01843428611755371 12400 217839.18982909256\n",
            "0.018649816513061523 12500 450807.5763659906\n",
            "0.018536806106567383 12600 215753.1334849787\n",
            "0.018637657165527344 12700 274053.9781252794\n",
            "0.018619775772094727 12800 219557.6352649911\n",
            "0.01841425895690918 12900 219366.85753141216\n",
            "0.018832683563232422 13000 267590.9167278286\n",
            "0.018539905548095703 13100 219209.12278884466\n",
            "0.018620967864990234 13200 1242324.6987015728\n",
            "0.018576622009277344 13300 219389.29125262902\n",
            "0.018996000289916992 13400 218960.5253505855\n",
            "0.01865839958190918 13500 232816.54511002157\n",
            "0.021743297576904297 13600 218827.92048103714\n",
            "0.018665790557861328 13700 218729.30869986888\n",
            "0.018795251846313477 13800 235210.95220294112\n",
            "0.019816875457763672 13900 218598.99551469783\n",
            "0.018517017364501953 14000 219004.7097413519\n",
            "0.018695831298828125 14100 220526.22660596628\n",
            "0.018825769424438477 14200 218503.32678743472\n",
            "0.01860523223876953 14300 19760819.382488478\n",
            "0.01853489875793457 14400 218532.72059319913\n",
            "0.0186917781829834 14500 218382.36557691672\n",
            "0.021577835083007812 14600 2391836.7900421275\n",
            "0.0188753604888916 14700 218279.335454694\n",
            "0.018573522567749023 14800 218159.95554922125\n",
            "0.018437862396240234 14900 850538.2966701217\n",
            "0.018317461013793945 15000 218134.5319880268\n",
            "0.01878070831298828 15100 218015.16765883187\n",
            "0.01866602897644043 15200 228080.1251113327\n",
            "0.01859593391418457 15300 218047.25514247586\n",
            "0.01865983009338379 15400 218064.6629498491\n",
            "0.01854419708251953 15500 220286.72958723354\n",
            "0.01989126205444336 15600 217982.37812568198\n",
            "0.02321171760559082 15700 228274.54034817606\n",
            "0.018590211868286133 15800 220385.88371389854\n",
            "0.018678903579711914 15900 217914.45289621697\n",
            "0.018689870834350586 16000 218077.69554639768\n",
            "0.018647432327270508 16100 222990.39799329013\n",
            "0.01851487159729004 16200 217712.2113590458\n",
            "0.018660783767700195 16300 217654.49403934795\n",
            "0.018407821655273438 16400 255699.23565950058\n",
            "0.022212982177734375 16500 217545.03459117824\n",
            "0.018688440322875977 16600 217486.18562574132\n",
            "0.018371105194091797 16700 464174.91713577736\n",
            "0.01857304573059082 16800 217359.93739287308\n",
            "0.023125410079956055 16900 217280.9689815046\n",
            "0.01861119270324707 17000 266283.1572282512\n",
            "0.018783092498779297 17100 217971.41565383566\n",
            "0.019251346588134766 17200 1507135.4623784134\n",
            "0.01874089241027832 17300 217895.88425461773\n",
            "0.018601179122924805 17400 218691.35322239238\n",
            "0.018501758575439453 17500 224472.40439806608\n",
            "0.01899552345275879 17600 10184450.959176537\n",
            "0.01855921745300293 17700 217343.9726848709\n",
            "0.018589258193969727 17800 2295047.986056351\n",
            "0.018535852432250977 17900 216693.21613783704\n",
            "0.01899576187133789 18000 221760.63349005047\n",
            "0.02070784568786621 18100 216369.51947013248\n",
            "0.018479108810424805 18200 219323.55064470333\n",
            "0.01866006851196289 18300 218261.34329503987\n",
            "0.018541574478149414 18400 215797.45149710108\n",
            "0.0185086727142334 18500 221522.46230872383\n",
            "0.018590450286865234 18600 216676.31392165\n",
            "0.01864457130432129 18700 13350535.837566532\n",
            "0.018541812896728516 18800 215796.07608777704\n",
            "0.023517608642578125 18900 215246.96387568812\n",
            "0.01847243309020996 19000 19586764.869505756\n",
            "0.018568754196166992 19100 215049.12876464668\n",
            "0.018496274948120117 19200 214270.15757751637\n",
            "0.018666505813598633 19300 471680.7104347252\n",
            "0.019739151000976562 19400 213705.5885383458\n",
            "0.018743038177490234 19500 213546.2050808188\n",
            "0.018524646759033203 19600 252800.04274191376\n",
            "0.02879047393798828 19700 299254.9733572749\n",
            "0.018597126007080078 19800 407267.6501144679\n",
            "0.018517732620239258 19900 627491.4540999087\n",
            "0.018668174743652344 20000 305964.14993996196\n",
            "0.019016504287719727 20100 201272.77753639081\n",
            "0.018619775772094727 20200 194963.0108869835\n",
            "0.018634319305419922 20300 197842.2510554957\n",
            "0.01946878433227539 20400 206263.2848395177\n",
            "0.018683671951293945 20500 185882.8052231037\n",
            "0.01856517791748047 20600 1105750.4975953167\n",
            "0.018458843231201172 20700 176730.9110443151\n",
            "0.018690824508666992 20800 2537487.662204404\n",
            "0.018549680709838867 20900 171721.65120443882\n",
            "0.018518686294555664 21000 2054740.1802822556\n",
            "0.01855182647705078 21100 157969.8575194915\n",
            "0.018912076950073242 21200 590113.5559190168\n",
            "0.018404245376586914 21300 155282.83590892173\n",
            "0.018647193908691406 21400 146952.1849410064\n",
            "0.01858210563659668 21500 2910498.7756390423\n",
            "0.021422147750854492 21600 137132.04944845792\n",
            "0.018543481826782227 21700 135125.72251801137\n",
            "0.01857447624206543 21800 134469.37097105494\n",
            "0.020750761032104492 21900 132256.27207185025\n",
            "0.018771886825561523 22000 359518.8725520821\n",
            "0.018823623657226562 22100 134991.61792975612\n",
            "0.019872188568115234 22200 128370.69042942044\n",
            "0.01866292953491211 22300 4060421.857383058\n",
            "0.018441200256347656 22400 125951.41745670774\n",
            "0.018472909927368164 22500 124172.3822051806\n",
            "0.01864790916442871 22600 496313.1802646779\n",
            "0.01866888999938965 22700 121981.57637504068\n",
            "0.01901984214782715 22800 138072.38204186733\n",
            "0.021048307418823242 22900 119866.52087935005\n",
            "0.018744468688964844 23000 118431.67377397079\n",
            "0.01887679100036621 23100 167540.98198241566\n",
            "0.018619775772094727 23200 143739.30900153134\n",
            "0.01871013641357422 23300 1292718.116506406\n",
            "0.018702030181884766 23400 118321.52794179531\n",
            "0.018431425094604492 23500 146501.0320729821\n",
            "0.01857781410217285 23600 116882.71297945268\n",
            "0.01825404167175293 23700 131074.83141891856\n",
            "0.018443822860717773 23800 115669.49135939006\n",
            "0.018486976623535156 23900 117207.65721140499\n",
            "0.018640995025634766 24000 114953.6859722834\n",
            "0.018571138381958008 24100 5565860.4015362235\n",
            "0.01848292350769043 24200 114360.15181536599\n",
            "0.018691062927246094 24300 114981.42953302037\n",
            "0.018910884857177734 24400 114857.43165657538\n",
            "0.018634796142578125 24500 113199.90941569574\n",
            "0.01913738250732422 24600 115187.81622028741\n",
            "0.01886463165283203 24700 112700.36666365681\n",
            "0.018558025360107422 24800 1262965.854374429\n",
            "0.018537044525146484 24900 112381.64154278064\n",
            "0.018497943878173828 25000 110938.73184937968\n",
            "0.018465280532836914 25100 147405.2781782509\n",
            "0.018339872360229492 25200 110139.11914050611\n",
            "0.020252704620361328 25300 527934.9377825041\n",
            "0.018628835678100586 25400 110237.40171739912\n",
            "0.018482685089111328 25500 128473.02969033412\n",
            "0.018538236618041992 25600 112132.09834140286\n",
            "0.01852893829345703 25700 132415.93655916024\n",
            "0.01859593391418457 25800 111984.19834550642\n",
            "0.018617868423461914 25900 180938.98579298815\n",
            "0.0188596248626709 26000 111129.7206438286\n",
            "0.018555164337158203 26100 1099976.3495908927\n",
            "0.018727540969848633 26200 110788.88830663452\n",
            "0.018659591674804688 26300 109793.23118721863\n",
            "0.018549203872680664 26400 116522.47200037149\n",
            "0.01852107048034668 26500 111207.92739522003\n",
            "0.018415212631225586 26600 12727184.506366894\n",
            "0.018654823303222656 26700 110380.16358952504\n",
            "0.018523693084716797 26800 110004.02714688082\n",
            "0.018694162368774414 26900 261404.7199753189\n",
            "0.018650293350219727 27000 108475.82215027804\n",
            "0.018400907516479492 27100 10681291.0609611\n",
            "0.018605947494506836 27200 108711.1488672526\n",
            "0.018973112106323242 27300 108064.05614105964\n",
            "0.018774986267089844 27400 115427.8667783264\n",
            "0.01859283447265625 27500 106783.07384628766\n",
            "0.018482208251953125 27600 181756.38706794553\n",
            "0.01824665069580078 27700 130946.20443655498\n",
            "0.018659591674804688 27800 105714.34929239292\n",
            "0.0186617374420166 27900 318566.3273876462\n",
            "0.018465042114257812 28000 105288.69941125414\n",
            "0.018588542938232422 28100 109642.86820564287\n",
            "0.018775463104248047 28200 279863.6281019853\n",
            "0.018520355224609375 28300 127220.47423853984\n",
            "0.018659591674804688 28400 103982.05031500052\n",
            "0.01882767677307129 28500 961836.1213373421\n",
            "0.018578290939331055 28600 110571.8356140565\n",
            "0.018639326095581055 28700 104335.12261531506\n",
            "0.018362760543823242 28800 103195.93615222558\n",
            "0.018555402755737305 28900 104800.47699466263\n",
            "0.01847672462463379 29000 331639.1047879691\n",
            "0.01838064193725586 29100 115853.86889784422\n",
            "0.018526077270507812 29200 103498.8442222045\n",
            "0.018643617630004883 29300 3163062.7007360826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8FLynwWGoX-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSaHDBl2pVMm",
        "outputId": "109fc926-cbb0-4917-aa09-6dcc785baae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!git clone https://github.com/cha0stooo/f16flyingdatasets.git\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'f16flyingdatasets'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 11 (delta 3), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}